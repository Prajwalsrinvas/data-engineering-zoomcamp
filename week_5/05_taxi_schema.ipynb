{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c1d0c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession, types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a01b0555-a1e4-47df-866c-69dbb368aea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mdata\u001b[0m\n",
      "└── \u001b[01;34mraw\u001b[0m\n",
      "    ├── \u001b[01;34mgreen\u001b[0m\n",
      "    │   ├── \u001b[01;34m2020\u001b[0m\n",
      "    │   │   ├── \u001b[01;34m01\u001b[0m\n",
      "    │   │   │   └── \u001b[01;31mgreen_tripdata_2020_01.csv.gz\u001b[0m\n",
      "    │   │   ├── \u001b[01;34m02\u001b[0m\n",
      "    │   │   │   └── \u001b[01;31mgreen_tripdata_2020_02.csv.gz\u001b[0m\n",
      "    │   │   ├── \u001b[01;34m03\u001b[0m\n",
      "    │   │   │   └── \u001b[01;31mgreen_tripdata_2020_03.csv.gz\u001b[0m\n",
      "    │   │   ├── \u001b[01;34m04\u001b[0m\n",
      "    │   │   │   └── \u001b[01;31mgreen_tripdata_2020_04.csv.gz\u001b[0m\n",
      "    │   │   ├── \u001b[01;34m05\u001b[0m\n",
      "    │   │   │   └── \u001b[01;31mgreen_tripdata_2020_05.csv.gz\u001b[0m\n",
      "    │   │   ├── \u001b[01;34m06\u001b[0m\n",
      "    │   │   │   └── \u001b[01;31mgreen_tripdata_2020_06.csv.gz\u001b[0m\n",
      "    │   │   ├── \u001b[01;34m07\u001b[0m\n",
      "    │   │   │   └── \u001b[01;31mgreen_tripdata_2020_07.csv.gz\u001b[0m\n",
      "    │   │   ├── \u001b[01;34m08\u001b[0m\n",
      "    │   │   │   └── \u001b[01;31mgreen_tripdata_2020_08.csv.gz\u001b[0m\n",
      "    │   │   ├── \u001b[01;34m09\u001b[0m\n",
      "    │   │   │   └── \u001b[01;31mgreen_tripdata_2020_09.csv.gz\u001b[0m\n",
      "    │   │   ├── \u001b[01;34m10\u001b[0m\n",
      "    │   │   │   └── \u001b[01;31mgreen_tripdata_2020_10.csv.gz\u001b[0m\n",
      "    │   │   ├── \u001b[01;34m11\u001b[0m\n",
      "    │   │   │   └── \u001b[01;31mgreen_tripdata_2020_11.csv.gz\u001b[0m\n",
      "    │   │   └── \u001b[01;34m12\u001b[0m\n",
      "    │   │       └── \u001b[01;31mgreen_tripdata_2020_12.csv.gz\u001b[0m\n",
      "    │   └── \u001b[01;34m2021\u001b[0m\n",
      "    │       ├── \u001b[01;34m01\u001b[0m\n",
      "    │       │   └── \u001b[01;31mgreen_tripdata_2021_01.csv.gz\u001b[0m\n",
      "    │       ├── \u001b[01;34m02\u001b[0m\n",
      "    │       │   └── \u001b[01;31mgreen_tripdata_2021_02.csv.gz\u001b[0m\n",
      "    │       ├── \u001b[01;34m03\u001b[0m\n",
      "    │       │   └── \u001b[01;31mgreen_tripdata_2021_03.csv.gz\u001b[0m\n",
      "    │       ├── \u001b[01;34m04\u001b[0m\n",
      "    │       │   └── \u001b[01;31mgreen_tripdata_2021_04.csv.gz\u001b[0m\n",
      "    │       ├── \u001b[01;34m05\u001b[0m\n",
      "    │       │   └── \u001b[01;31mgreen_tripdata_2021_05.csv.gz\u001b[0m\n",
      "    │       ├── \u001b[01;34m06\u001b[0m\n",
      "    │       │   └── \u001b[01;31mgreen_tripdata_2021_06.csv.gz\u001b[0m\n",
      "    │       └── \u001b[01;34m07\u001b[0m\n",
      "    │           └── \u001b[01;31mgreen_tripdata_2021_07.csv.gz\u001b[0m\n",
      "    └── \u001b[01;34myellow\u001b[0m\n",
      "        ├── \u001b[01;34m2020\u001b[0m\n",
      "        │   ├── \u001b[01;34m01\u001b[0m\n",
      "        │   │   └── \u001b[01;31myellow_tripdata_2020_01.csv.gz\u001b[0m\n",
      "        │   ├── \u001b[01;34m02\u001b[0m\n",
      "        │   │   └── \u001b[01;31myellow_tripdata_2020_02.csv.gz\u001b[0m\n",
      "        │   ├── \u001b[01;34m03\u001b[0m\n",
      "        │   │   └── \u001b[01;31myellow_tripdata_2020_03.csv.gz\u001b[0m\n",
      "        │   ├── \u001b[01;34m04\u001b[0m\n",
      "        │   │   └── \u001b[01;31myellow_tripdata_2020_04.csv.gz\u001b[0m\n",
      "        │   ├── \u001b[01;34m05\u001b[0m\n",
      "        │   │   └── \u001b[01;31myellow_tripdata_2020_05.csv.gz\u001b[0m\n",
      "        │   ├── \u001b[01;34m06\u001b[0m\n",
      "        │   │   └── \u001b[01;31myellow_tripdata_2020_06.csv.gz\u001b[0m\n",
      "        │   ├── \u001b[01;34m07\u001b[0m\n",
      "        │   │   └── \u001b[01;31myellow_tripdata_2020_07.csv.gz\u001b[0m\n",
      "        │   ├── \u001b[01;34m08\u001b[0m\n",
      "        │   │   └── \u001b[01;31myellow_tripdata_2020_08.csv.gz\u001b[0m\n",
      "        │   ├── \u001b[01;34m09\u001b[0m\n",
      "        │   │   └── \u001b[01;31myellow_tripdata_2020_09.csv.gz\u001b[0m\n",
      "        │   ├── \u001b[01;34m10\u001b[0m\n",
      "        │   │   └── \u001b[01;31myellow_tripdata_2020_10.csv.gz\u001b[0m\n",
      "        │   ├── \u001b[01;34m11\u001b[0m\n",
      "        │   │   └── \u001b[01;31myellow_tripdata_2020_11.csv.gz\u001b[0m\n",
      "        │   └── \u001b[01;34m12\u001b[0m\n",
      "        │       └── \u001b[01;31myellow_tripdata_2020_12.csv.gz\u001b[0m\n",
      "        └── \u001b[01;34m2021\u001b[0m\n",
      "            ├── \u001b[01;34m01\u001b[0m\n",
      "            │   └── \u001b[01;31myellow_tripdata_2021_01.csv.gz\u001b[0m\n",
      "            ├── \u001b[01;34m02\u001b[0m\n",
      "            │   └── \u001b[01;31myellow_tripdata_2021_02.csv.gz\u001b[0m\n",
      "            ├── \u001b[01;34m03\u001b[0m\n",
      "            │   └── \u001b[01;31myellow_tripdata_2021_03.csv.gz\u001b[0m\n",
      "            ├── \u001b[01;34m04\u001b[0m\n",
      "            │   └── \u001b[01;31myellow_tripdata_2021_04.csv.gz\u001b[0m\n",
      "            ├── \u001b[01;34m05\u001b[0m\n",
      "            │   └── \u001b[01;31myellow_tripdata_2021_05.csv.gz\u001b[0m\n",
      "            ├── \u001b[01;34m06\u001b[0m\n",
      "            │   └── \u001b[01;31myellow_tripdata_2021_06.csv.gz\u001b[0m\n",
      "            └── \u001b[01;34m07\u001b[0m\n",
      "                └── \u001b[01;31myellow_tripdata_2021_07.csv.gz\u001b[0m\n",
      "\n",
      "45 directories, 38 files\n"
     ]
    }
   ],
   "source": [
    "!tree data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96a248f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/09 17:04:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a84c6c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "green_schema = types.StructType(\n",
    "    [\n",
    "        types.StructField(\"VendorID\", types.IntegerType(), True),\n",
    "        types.StructField(\"lpep_pickup_datetime\", types.TimestampType(), True),\n",
    "        types.StructField(\"lpep_dropoff_datetime\", types.TimestampType(), True),\n",
    "        types.StructField(\"store_and_fwd_flag\", types.StringType(), True),\n",
    "        types.StructField(\"RatecodeID\", types.IntegerType(), True),\n",
    "        types.StructField(\"PULocationID\", types.IntegerType(), True),\n",
    "        types.StructField(\"DOLocationID\", types.IntegerType(), True),\n",
    "        types.StructField(\"passenger_count\", types.IntegerType(), True),\n",
    "        types.StructField(\"trip_distance\", types.DoubleType(), True),\n",
    "        types.StructField(\"fare_amount\", types.DoubleType(), True),\n",
    "        types.StructField(\"extra\", types.DoubleType(), True),\n",
    "        types.StructField(\"mta_tax\", types.DoubleType(), True),\n",
    "        types.StructField(\"tip_amount\", types.DoubleType(), True),\n",
    "        types.StructField(\"tolls_amount\", types.DoubleType(), True),\n",
    "        types.StructField(\"ehail_fee\", types.DoubleType(), True),\n",
    "        types.StructField(\"improvement_surcharge\", types.DoubleType(), True),\n",
    "        types.StructField(\"total_amount\", types.DoubleType(), True),\n",
    "        types.StructField(\"payment_type\", types.IntegerType(), True),\n",
    "        types.StructField(\"trip_type\", types.IntegerType(), True),\n",
    "        types.StructField(\"congestion_surcharge\", types.DoubleType(), True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12207397-2910-410c-827b-41d2ebe39681",
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_schema = types.StructType(\n",
    "    [\n",
    "        types.StructField(\"VendorID\", types.IntegerType(), True),\n",
    "        types.StructField(\"tpep_pickup_datetime\", types.TimestampType(), True),\n",
    "        types.StructField(\"tpep_dropoff_datetime\", types.TimestampType(), True),\n",
    "        types.StructField(\"passenger_count\", types.IntegerType(), True),\n",
    "        types.StructField(\"trip_distance\", types.DoubleType(), True),\n",
    "        types.StructField(\"RatecodeID\", types.IntegerType(), True),\n",
    "        types.StructField(\"store_and_fwd_flag\", types.StringType(), True),\n",
    "        types.StructField(\"PULocationID\", types.IntegerType(), True),\n",
    "        types.StructField(\"DOLocationID\", types.IntegerType(), True),\n",
    "        types.StructField(\"payment_type\", types.IntegerType(), True),\n",
    "        types.StructField(\"fare_amount\", types.DoubleType(), True),\n",
    "        types.StructField(\"extra\", types.DoubleType(), True),\n",
    "        types.StructField(\"mta_tax\", types.DoubleType(), True),\n",
    "        types.StructField(\"tip_amount\", types.DoubleType(), True),\n",
    "        types.StructField(\"tolls_amount\", types.DoubleType(), True),\n",
    "        types.StructField(\"improvement_surcharge\", types.DoubleType(), True),\n",
    "        types.StructField(\"total_amount\", types.DoubleType(), True),\n",
    "        types.StructField(\"congestion_surcharge\", types.DoubleType(), True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f7e0cb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2020/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2020/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2020/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2020/4\n",
      "processing data for 2020/5\n",
      "processing data for 2020/6\n",
      "processing data for 2020/7\n",
      "processing data for 2020/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2020/9\n",
      "processing data for 2020/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2020/11\n",
      "processing data for 2020/12\n"
     ]
    }
   ],
   "source": [
    "year = 2020\n",
    "\n",
    "for month in range(1, 13):\n",
    "    print(f\"processing data for {year}/{month}\")\n",
    "\n",
    "    input_path = f\"data/raw/green/{year}/{month:02d}/\"\n",
    "    output_path = f\"data/pq/green/{year}/{month:02d}/\"\n",
    "\n",
    "    df_green = spark.read.option(\"header\", \"true\").schema(green_schema).csv(input_path)\n",
    "\n",
    "    df_green.repartition(4).write.parquet(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96ac2ad7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2021/1\n",
      "processing data for 2021/2\n",
      "processing data for 2021/3\n",
      "processing data for 2021/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2021/5\n",
      "processing data for 2021/6\n",
      "processing data for 2021/7\n",
      "processing data for 2021/8\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/home/prajwal/code/data-engineering-zoomcamp/week_5_batch_processing/code/data/raw/green/2021/08",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12032/965965834.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0moutput_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"data/pq/green/{year}/{month:02d}/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdf_green\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgreen_schema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mdf_green\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/home/prajwal/code/data-engineering-zoomcamp/week_5_batch_processing/code/data/raw/green/2021/08"
     ]
    }
   ],
   "source": [
    "year = 2021\n",
    "\n",
    "for month in range(1, 13):\n",
    "    print(f\"processing data for {year}/{month}\")\n",
    "\n",
    "    input_path = f\"data/raw/green/{year}/{month:02d}/\"\n",
    "    output_path = f\"data/pq/green/{year}/{month:02d}/\"\n",
    "\n",
    "    df_green = spark.read.option(\"header\", \"true\").schema(green_schema).csv(input_path)\n",
    "\n",
    "    df_green.repartition(4).write.parquet(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff4265d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e982d29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19326bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2020/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2020/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2020/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2020/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2020/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2020/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2020/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2020/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2020/9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2020/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2020/11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2020/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "year = 2020\n",
    "\n",
    "for month in range(1, 13):\n",
    "    print(f\"processing data for {year}/{month}\")\n",
    "\n",
    "    input_path = f\"data/raw/yellow/{year}/{month:02d}/\"\n",
    "    output_path = f\"data/pq/yellow/{year}/{month:02d}/\"\n",
    "\n",
    "    df_yellow = (\n",
    "        spark.read.option(\"header\", \"true\").schema(yellow_schema).csv(input_path)\n",
    "    )\n",
    "\n",
    "    df_yellow.repartition(4).write.parquet(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aeca811a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2021/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2021/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2021/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2021/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2021/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2021/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2021/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 113:>                                                        (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2021/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/home/prajwal/code/data-engineering-zoomcamp/week_5_batch_processing/code/data/raw/yellow/2021/08",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12032/3427592344.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     df_yellow = (\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myellow_schema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     )\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/home/prajwal/code/data-engineering-zoomcamp/week_5_batch_processing/code/data/raw/yellow/2021/08"
     ]
    }
   ],
   "source": [
    "year = 2021\n",
    "\n",
    "for month in range(1, 13):\n",
    "    print(f\"processing data for {year}/{month}\")\n",
    "\n",
    "    input_path = f\"data/raw/yellow/{year}/{month:02d}/\"\n",
    "    output_path = f\"data/pq/yellow/{year}/{month:02d}/\"\n",
    "\n",
    "    df_yellow = (\n",
    "        spark.read.option(\"header\", \"true\").schema(yellow_schema).csv(input_path)\n",
    "    )\n",
    "\n",
    "    df_yellow.repartition(4).write.parquet(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7daa188-99b2-4fcd-b895-e3b4f6203f1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
